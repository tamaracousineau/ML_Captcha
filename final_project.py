# -*- coding: utf-8 -*-
"""Final_Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qKt9Bk6ZbAOTf5K6vQHp_OOuAA19JvIo

**SOLVING CAPTCHA **

Train a neural network to solve CAPTCHA puzzles 
For each set of images, being able to categorized each based on the content.
Car, bus, bike â€¦ 
Using convolutional neural network (CNN) for classification
Why CNN?
High accuracy
Detects important features without human supervision
Hierarchical model that build a network similar to a funnel 
Using TensorFlow library in python
"""

# ! pip install opendatasets

import numpy as np
from keras.datasets import cifar100
from keras.constraints import maxnorm
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.models import Sequential
from keras.utils import np_utils

import matplotlib
import matplotlib.pyplot as plt

(X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode="fine")

# simple model to try a basic implementation
simple_layers = [
      Flatten(),
      Dense(25, activation="relu"),
      Dense(35, activation="relu"),
      Dense(45, activation="relu"),
      Dense(50, activation="relu"),
      Dropout(0.05),
      Dense(50, activation="relu"),
      Dense(50, activation="relu"),
      Dense(45, activation="relu"),
      Dense(35, activation="relu"),
      Dense(25, activation="relu"),
      Dense(100, activation="softmax")
    ]

simple_model = Sequential(layers=simple_layers)

simple_model.compile(optimizer="adam", loss='sparse_categorical_crossentropy', metrics=['accuracy'])
simple_model.fit(X_train, y_train, epochs=5)
print(simple_model.summary())

simple_scores = simple_model.evaluate(X_test, y_test)
print("Accuracy:", simple_scores[0])

from google.colab import drive
drive.mount('/content/drive')

from tensorflow.keras.datasets import cifar100
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.losses import sparse_categorical_crossentropy
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

# Model configuration
batch_size = 50
img_width, img_height, img_num_channels = 32, 32, 3
loss_function = sparse_categorical_crossentropy
no_classes = 100
no_epochs = 100
optimizer = Adam()
validation_split = 0.2
verbosity = 1

# Load CIFAR-100 data
(input_train, target_train), (input_test, target_test) = cifar100.load_data()

# Determine shape of the data
input_shape = (img_width, img_height, img_num_channels)

print(type(target_train))
print(target_train)


# Parse numbers as floats
input_train = input_train.astype('float32')
input_test = input_test.astype('float32')

# Normalize data
input_train = input_train / 255
input_test = input_test / 255



# Create the model
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(512, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(no_classes, activation='softmax'))

# Compile the model
model.compile(loss=loss_function,
              optimizer=optimizer,
              metrics=['accuracy'])

# Fit data to model
history = model.fit(input_train, target_train,
            batch_size=batch_size,
            epochs=no_epochs,
            verbose=verbosity,
            validation_split=validation_split)

# Generate generalization metrics
score = model.evaluate(input_test, target_test, verbose=0)
print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')

# Visualize history
# Plot history: Loss
plt.plot(history.history['val_loss'])
plt.title('Validation loss history')
plt.ylabel('Loss value')
plt.xlabel('No. epoch')
plt.show()

# Plot history: Accuracy
plt.plot(history.history['val_accuracy'])
plt.title('Validation accuracy history')
plt.ylabel('Accuracy value (%)')
plt.xlabel('No. epoch')
plt.show()

from os import listdir
import pandas as pd
import cv2

# Load CAPTCHA dataset (skip mountain, chimney, motorcycle)
min_images = 100
df = pd.DataFrame()

# BICYCLE
filepath = "drive/MyDrive/Large/Bicycle/"
files = listdir(filepath)
i = 0
for file in files:
  x = cv2.imread(filepath + file)
  y = "Bicycle"
  df_temp = pd.DataFrame([(x, y)], columns=["img", "label"])
  df = df.append(df_temp)
  if i == min_images: break
  i += 1

# BRIDGE
filepath = "drive/MyDrive/Large/Bridge/"
files = listdir(filepath)
i = 0
for file in files:
  x = cv2.imread(filepath + file)
  y = "Bridge"
  df_temp = pd.DataFrame([(x, y)], columns=["img", "label"])
  df = df.append(df_temp)
  if i == min_images: break
  i += 1

# BUS
filepath = "drive/MyDrive/Large/Bus/"
files = listdir(filepath)
print(files)
i = 0
for file in files:
  x = cv2.imread(filepath + file)
  print(x)
  y = "Bus"
  df_temp = pd.DataFrame([(x, y)], columns=["img", "label"])
  df = df.append(df_temp)
  if i == min_images: break
  i += 1


print(df.tail())

# Repeat model for our CAPTCHA dataset
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.losses import sparse_categorical_crossentropy
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt


# Model configuration
batch_size = 50
img_width, img_height, img_num_channels = 32, 32, 3
loss_function = sparse_categorical_crossentropy
no_classes = 100
no_epochs = 100
optimizer = Adam()
validation_split = 0.2
verbosity = 1



(input_train, target_train), (input_test, target_test) = 

# Determine shape of the data
input_shape = (img_width, img_height, img_num_channels)

# Parse numbers as floats
input_train = input_train.astype('float32')
input_test = input_test.astype('float32')

# Normalize data
input_train = input_train / 255
input_test = input_test / 255

# Create the model
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(512, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(no_classes, activation='softmax'))

# Compile the model
model.compile(loss=loss_function,
              optimizer=optimizer,
              metrics=['accuracy'])

# Fit data to model
history = model.fit(input_train, target_train,
            batch_size=batch_size,
            epochs=no_epochs,
            verbose=verbosity,
            validation_split=validation_split)

# Generate generalization metrics
score = model.evaluate(input_test, target_test, verbose=0)
print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')

# Visualize history
# Plot history: Loss
plt.plot(history.history['val_loss'])
plt.title('Validation loss history')
plt.ylabel('Loss value')
plt.xlabel('No. epoch')
plt.show()

# Plot history: Accuracy
plt.plot(history.history['val_accuracy'])
plt.title('Validation accuracy history')
plt.ylabel('Accuracy value (%)')
plt.xlabel('No. epoch')
plt.show()

# Attempt 1: OCR Handwriting dataset

import csv
import cv2

y_train = []
y_test = []
X_train = []
X_test = []


# load y_test data
with open('handwriting-recognitionocr/CSV/written_name_test.csv', mode ='r')as file:
  testFile = csv.reader(file)
  for line in testFile:
    y_test.append(line[1])

# load y_train data
with open('handwriting-recognitionocr/CSV/written_name_train.csv', mode ='r')as file:
  trainFile = csv.reader(file)
  for line in trainFile:
    y_train.append(line[1])

    # limit ourselves to the first 10,000 images so our program doesn't crash
    if(len(y_train) > 10000): break

# load test images into X_test
for i in range(1, len(y_test)):
  if i < 10:
    img = cv2.imread("handwriting-recognitionocr/test_v2/test/TEST_000" + str(i) + ".jpg")
    X_test.append(img)
  elif i < 100:
    img = cv2.imread("handwriting-recognitionocr/test_v2/test/TEST_00" + str(i) + ".jpg")
    X_test.append(img)
  elif i < 1000:
    img = cv2.imread("handwriting-recognitionocr/test_v2/test/TEST_0" + str(i) + ".jpg")
    X_test.append(img)    
  else:
    img = cv2.imread("handwriting-recognitionocr/test_v2/test/TEST_" + str(i) + ".jpg")
    X_test.append(img)

# load training images into X_train
for i in range(1, 10001):
  if i < 10:
    img = cv2.imread("handwriting-recognitionocr/train_v2/train/TRAIN_0000" + str(i) + ".jpg")
    X_train.append(img)
  elif i < 100:
    img = cv2.imread("handwriting-recognitionocr/train_v2/train/TRAIN_000" + str(i) + ".jpg")
    X_train.append(img)
  elif i < 1000:
    img = cv2.imread("handwriting-recognitionocr/train_v2/train/TRAIN_00" + str(i) + ".jpg")
    X_train.append(img)
  elif i < 10000:
    img = cv2.imread("handwriting-recognitionocr/train_v2/train/TRAIN_0" + str(i) + ".jpg")
    X_train.append(img)
  else:
    img = cv2.imread("handwriting-recognitionocr/train_v2/train/TRAIN_" + str(i) + ".jpg")
    X_train.append(img)


assert(len(X_test) == len(y_test) - 1)
assert(len(X_train) == len(y_train) - 1)

simple_layers = [
      Flatten(),
      Dense(150, activation="relu"),
      Dropout(.02),
      Dense(250, activation="relu"),
      Dropout(.05),
      Dense(10, activation="softmax")
    ]

simple_model = Sequential(layers=simple_layers)

simple_model.compile(optimizer="adam", loss='sparse_categorical_crossentropy', metrics=['accuracy'])
simple_model.fit(X_train, y_train, epochs=5)
simple_model.summary()



